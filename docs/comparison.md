# ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” DQN, PPO, SACì˜ ì°¨ì´ì ê³¼ ê° ì•Œê³ ë¦¬ì¦˜ì„ ì–¸ì œ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…í•©ë‹ˆë‹¤.

## í•œëˆˆì— ë³´ëŠ” ë¹„êµí‘œ

| íŠ¹ì§• | DQN | PPO | SAC |
|-----|-----|-----|-----|
| **ê³„ì—´** | Value-based | Actor-Critic | Actor-Critic |
| **On/Off-policy** | Off-policy | On-policy | Off-policy |
| **Action Space** | Discreteë§Œ | ë‘˜ ë‹¤ | Continuousë§Œ |
| **ìƒ˜í”Œ íš¨ìœ¨ì„±** | ì¤‘ê°„~ë†’ìŒ | ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ |
| **ì•ˆì •ì„±** | ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ | ë†’ìŒ |
| **êµ¬í˜„ ë‚œì´ë„** | ì‰¬ì›€ | ì‰¬ì›€ | ì¤‘ê°„ |
| **í•™ìŠµ ì†ë„** | ë¹ ë¦„ | ì¤‘ê°„ | ë¹ ë¦„ |
| **íƒìƒ‰ ì „ëµ** | Îµ-greedy | Gaussian noise | Entropy max |
| **ë°ì´í„° ì¬ì‚¬ìš©** | Replay buffer | ì œí•œì  | Replay buffer |

## ì§„í™” ê³¼ì •

```
ğŸ“š ê°•í™”í•™ìŠµì˜ ì§„í™”

1. Value-based (DQN)
   "Q ê°’ì„ í•™ìŠµí•˜ì"
   â†“
   ë¬¸ì œ: Continuous action ë¶ˆê°€

2. Policy-based (REINFORCE)
   "ì •ì±…ì„ ì§ì ‘ í•™ìŠµí•˜ì"
   â†“
   ë¬¸ì œ: ë¶„ì‚°ì´ ë„ˆë¬´ í¼

3. Actor-Critic (A2C/A3C)
   "ì •ì±… + ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ê°™ì´ ì“°ì"
   â†“
   ë¬¸ì œ: ì—¬ì „íˆ ë¶ˆì•ˆì •

4. Trust Region (PPO)
   "ì •ì±… ë³€í™”ë¥¼ ì œí•œí•˜ì"
   âœ“ ì•ˆì •ì„± í™•ë³´
   â†“
   ë¬¸ì œ: ìƒ˜í”Œ íš¨ìœ¨ì„± ë‚®ìŒ (On-policy)

5. Maximum Entropy (SAC)
   "íƒìƒ‰ì„ ì ê·¹ì ìœ¼ë¡œ í•˜ì"
   âœ“ ìƒ˜í”Œ íš¨ìœ¨ì„± + ê°•ë ¥í•œ íƒìƒ‰
```

## ìƒì„¸ ë¹„êµ

### 1. Action Space

#### DQN: Discreteë§Œ âš ï¸
- Q(s, a)ë¥¼ ëª¨ë“  actionì— ëŒ€í•´ ê³„ì‚°
- ContinuousëŠ” ë¶ˆê°€ëŠ¥

**ì‚¬ìš© ê°€ëŠ¥**:
- CartPole (2ê°œ action)
- LunarLander (4ê°œ action)
- Breakout (4ê°œ action)

**ì‚¬ìš© ë¶ˆê°€**:
- BipedalWalker (4ì°¨ì› ì—°ì†)
- Ant (8ì°¨ì› ì—°ì†)

#### PPO: ë‘˜ ë‹¤ âœ…
- Discrete: Categorical distribution
- Continuous: Gaussian distribution

**ë²”ìš©ì„±ì´ ê°€ì¥ ë†’ìŒ!**

#### SAC: Continuousë§Œ âš ï¸
- Gaussian policy
- Squashed Gaussian (tanh)

**ì‚¬ìš© ê°€ëŠ¥**:
- BipedalWalker
- Ant

**ì‚¬ìš© ë¶ˆê°€**:
- CartPole, LunarLander, Breakout

### 2. ìƒ˜í”Œ íš¨ìœ¨ì„±

#### DQN: ì¤‘ê°„~ë†’ìŒ
- Replay buffer ì‚¬ìš©
- Off-policyë¡œ ê³¼ê±° ë°ì´í„° ì¬ì‚¬ìš©
- í•˜ì§€ë§Œ Q-learningì˜ í•œê³„

#### PPO: ì¤‘ê°„
- **On-policy**: í˜„ì¬ ì •ì±… ë°ì´í„°ë§Œ ì‚¬ìš©
- ë¯¸ë‹ˆë°°ì¹˜ ì¬ì‚¬ìš© (n_epochs)
- DQN/SACë³´ë‹¤ ëŠë¦¼

#### SAC: ë§¤ìš° ë†’ìŒ
- Replay buffer + Off-policy
- Entropy maximization
- **ê°€ì¥ ìƒ˜í”Œ íš¨ìœ¨ì !**

### ì‹¤í—˜ ê²°ê³¼ (BipedalWalker)

| ì•Œê³ ë¦¬ì¦˜ | ëª©í‘œ ë‹¬ì„± timesteps |
|---------|-------------------|
| PPO | ~2,000,000 |
| SAC | ~300,000 |

**SACê°€ ì•½ 6ë°° ë¹ ë¦„!**

### 3. ì•ˆì •ì„±

#### DQN: ì¤‘ê°„
- Overestimation bias
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë¯¼ê°
- Target networkë¡œ ì–´ëŠ ì •ë„ ì•ˆì •í™”

#### PPO: ë§¤ìš° ë†’ìŒ â­
- Clippingìœ¼ë¡œ ì •ì±… ë³€í™” ì œí•œ
- Trust region ë³´ì¥
- **ê°€ì¥ ì•ˆì •ì !**
- ì¬í˜„ì„± ì¢‹ìŒ

#### SAC: ë†’ìŒ
- Twin Q-networks
- Soft target update
- Automatic temperature tuning
- PPOë³´ë‹¤ëŠ” ì•½ê°„ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ

### 4. íƒìƒ‰ ì „ëµ

#### DQN: Îµ-greedy
```python
if random() < epsilon:
    action = random_action()
else:
    action = argmax(Q(s, a))
```
- ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ì 
- Epsilon decay í•„ìš”

#### PPO: Gaussian noise
```python
action = Ï€(s) + N(0, ÏƒÂ²)
```
- ì—°ì† í–‰ë™ì— ìì—°ìŠ¤ëŸ¬ì›€
- Entropy bonus ì˜µì…˜

#### SAC: Entropy maximization â­
```python
maximize: reward + Î± * entropy
```
- **ì ê·¹ì ì¸ íƒìƒ‰**
- Automatic tuning
- Local optimum íšŒí”¼

## í™˜ê²½ë³„ ê¶Œì¥ ì•Œê³ ë¦¬ì¦˜

### CartPole (Discrete, ì €ì°¨ì›)

| ì•Œê³ ë¦¬ì¦˜ | ì„±ëŠ¥ | í•™ìŠµ ì†ë„ | ê¶Œì¥ë„ |
|---------|------|----------|--------|
| DQN | â­â­â­â­â­ | ë¹ ë¦„ | âœ… |
| PPO | â­â­â­â­â­ | ë¹ ë¦„ | âœ… |
| SAC | âŒ | - | - |

**ì¶”ì²œ**: DQN (Value-based í•™ìŠµìš©) ë˜ëŠ” PPO (ì•ˆì •ì„±)

### LunarLander (Discrete, ì¤‘ê°„)

| ì•Œê³ ë¦¬ì¦˜ | ì„±ëŠ¥ | í•™ìŠµ ì†ë„ | ê¶Œì¥ë„ |
|---------|------|----------|--------|
| DQN | â­â­â­â­ | ì¤‘ê°„ | âœ… |
| PPO | â­â­â­â­â­ | ì¤‘ê°„ | âœ…âœ… |
| SAC | âŒ | - | - |

**ì¶”ì²œ**: PPO (ë” ì•ˆì •ì )

### BipedalWalker (Continuous, ì¤‘ê°„)

| ì•Œê³ ë¦¬ì¦˜ | ì„±ëŠ¥ | í•™ìŠµ ì†ë„ | ê¶Œì¥ë„ |
|---------|------|----------|--------|
| DQN | âŒ | - | - |
| PPO | â­â­â­â­ | ëŠë¦¼ | âœ… |
| SAC | â­â­â­â­â­ | ë¹ ë¦„ | âœ…âœ… |

**ì¶”ì²œ**: SAC (ìƒ˜í”Œ íš¨ìœ¨ì„±)

### Ant (Continuous, ê³ ì°¨ì›)

| ì•Œê³ ë¦¬ì¦˜ | ì„±ëŠ¥ | í•™ìŠµ ì†ë„ | ê¶Œì¥ë„ |
|---------|------|----------|--------|
| DQN | âŒ | - | - |
| PPO | â­â­â­â­ | ëŠë¦¼ | âœ… |
| SAC | â­â­â­â­â­ | ì¤‘ê°„ | âœ…âœ… |

**ì¶”ì²œ**: SAC (ê³ ì°¨ì› ì œì–´ ìµœì í™”)

### Breakout (Discrete, ì´ë¯¸ì§€)

| ì•Œê³ ë¦¬ì¦˜ | ì„±ëŠ¥ | í•™ìŠµ ì†ë„ | ê¶Œì¥ë„ |
|---------|------|----------|--------|
| DQN | â­â­â­â­â­ | ëŠë¦¼ | âœ…âœ… |
| PPO | â­â­â­â­ | ëŠë¦¼ | âœ… |
| SAC | âŒ | - | - |

**ì¶”ì²œ**: DQN (Atariì˜ í‘œì¤€)

## ì˜ì‚¬ê²°ì • íŠ¸ë¦¬

```
â”Œâ”€ Action space?
â”‚
â”œâ”€ Discrete
â”‚  â”‚
â”‚  â”œâ”€ ì´ë¯¸ì§€ ì…ë ¥?
â”‚  â”‚  â”œâ”€ Yes â†’ DQN (CnnPolicy)
â”‚  â”‚  â””â”€ No  â†’ DQN ë˜ëŠ” PPO
â”‚  â”‚
â”‚  â””â”€ ì•ˆì •ì„± ì¤‘ìš”?
â”‚     â”œâ”€ Yes â†’ PPO
â”‚     â””â”€ No  â†’ DQN
â”‚
â””â”€ Continuous
   â”‚
   â”œâ”€ ìƒ˜í”Œ íš¨ìœ¨ì„± ì¤‘ìš”?
   â”‚  â”œâ”€ Yes â†’ SAC
   â”‚  â””â”€ No  â†’ PPO
   â”‚
   â””â”€ ê³ ì°¨ì› (>4D)?
      â”œâ”€ Yes â†’ SAC
      â””â”€ No  â†’ PPO ë˜ëŠ” SAC
```

## ì‹¤ì „ ê°€ì´ë“œ

### ì²˜ìŒ ì‹œì‘í•˜ëŠ” ê²½ìš°
1. **PPO**ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
2. Discreteë©´ **DQN**ë„ ì‹œë„
3. Continuousë©´ **SAC**ë„ ì‹œë„

### ì„±ëŠ¥ì´ ì¤‘ìš”í•œ ê²½ìš°
- Discrete: **DQN** + **PPO** ë¹„êµ
- Continuous: **SAC**

### ì•ˆì •ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°
- **PPO** ìš°ì„ 

### ìƒ˜í”Œ íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°
- Discrete: **DQN**
- Continuous: **SAC**

### ì‹œë®¬ë ˆì´ì…˜ ë¹„ìš©ì´ ë†’ì€ ê²½ìš°
- **SAC** ë˜ëŠ” **DQN** (Off-policy)

## í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„

| ì•Œê³ ë¦¬ì¦˜ | ë¯¼ê°ë„ | íŠœë‹ ë‚œì´ë„ |
|---------|--------|-----------|
| DQN | ì¤‘ê°„ | ì¤‘ê°„ |
| PPO | ë‚®ìŒ | ì‰¬ì›€ â­ |
| SAC | ë‚®ìŒ | ì‰¬ì›€ (auto) â­ |

**PPOì™€ SACëŠ” ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œë„ ì˜ ì‘ë™í•©ë‹ˆë‹¤!**

## í•™ìŠµ ê³¡ì„  íŠ¹ì§•

### DQN
```
Reward
  â”‚    â•±â”€â”€
  â”‚   â•±
  â”‚  â•±
  â”‚ â•±
  â””â”€â”€â”€â”€â”€â”€ Steps
```
- ì´ˆë°˜ íƒìƒ‰ í›„ ë¹ ë¥¸ ìƒìŠ¹
- ì¤‘ê°„ ì •ë„ ë³€ë™ì„±
- Q-learning íŠ¹ìœ ì˜ íŒ¨í„´

### PPO
```
Reward
  â”‚     â”Œâ”€â”€â”€
  â”‚    â•±
  â”‚   â•±
  â”‚  â•±
  â””â”€â”€â”€â”€â”€â”€ Steps
```
- ì•ˆì •ì ì¸ ìƒìŠ¹
- ë‚®ì€ ë³€ë™ì„± â­
- ì˜ˆì¸¡ ê°€ëŠ¥í•œ ìˆ˜ë ´

### SAC
```
Reward
  â”‚      â•±â”€
  â”‚     â•±
  â”‚   â•±â•±
  â”‚ â•±â•±
  â””â”€â”€â”€â”€â”€â”€ Steps
```
- ì´ˆë°˜ íƒìƒ‰ (ë‚®ì€ ë³´ìƒ)
- ê¸‰ê²©í•œ ìƒìŠ¹
- ë†’ì€ ìµœì¢… ì„±ëŠ¥

## ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| ì•Œê³ ë¦¬ì¦˜ | ë©”ëª¨ë¦¬ | Replay Buffer |
|---------|--------|--------------|
| DQN | ì¤‘ê°„ | 50K~100K |
| PPO | ë‚®ìŒ | ì—†ìŒ |
| SAC | ë†’ìŒ | 1M |

**ë©”ëª¨ë¦¬ ì œí•œ ìˆìœ¼ë©´**: PPO

## ê³„ì‚° ë¹„ìš©

| ì•Œê³ ë¦¬ì¦˜ | Forward Pass | Backward Pass |
|---------|-------------|--------------|
| DQN | 1 network | 1 network |
| PPO | 2 networks | 2 networks |
| SAC | 3 networks | 3 networks |

**ê³„ì‚° ìì› ì œí•œ ìˆìœ¼ë©´**: DQN ë˜ëŠ” PPO

## ì‹¤í—˜ ì„¤ê³„

### PPO vs SAC (BipedalWalker)

**ëª©ì **: ìƒ˜í”Œ íš¨ìœ¨ì„± ë¹„êµ

**ë°©ë²•**:
1. ê°™ì€ í™˜ê²½ì—ì„œ ë‘ ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ
2. TensorBoardë¡œ í•™ìŠµ ê³¡ì„  ë¹„êµ
3. ëª©í‘œ ì„±ëŠ¥ ë„ë‹¬ timesteps ì¸¡ì •

**ì˜ˆìƒ ê²°ê³¼**:
- SACê°€ 5~10ë°° ë¹ ë¦„
- ìµœì¢… ì„±ëŠ¥ì€ ìœ ì‚¬

### DQN vs PPO (CartPole)

**ëª©ì **: Value-based vs Policy-based ë¹„êµ

**ë°©ë²•**:
1. ë‘ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ CartPole í•™ìŠµ
2. í•™ìŠµ ì•ˆì •ì„± ë¹„êµ
3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¹„êµ

**ì˜ˆìƒ ê²°ê³¼**:
- ì„±ëŠ¥ ìœ ì‚¬
- PPOê°€ ë” ì•ˆì •ì 

## ê²°ë¡ 

### DQN
- âœ… Discrete actionì˜ í‘œì¤€
- âœ… Atari ê²Œì„
- âŒ Continuous ë¶ˆê°€

### PPO
- âœ… ê°€ì¥ ì•ˆì •ì 
- âœ… ëª¨ë“  í™˜ê²½ ì§€ì›
- âœ… **ë²”ìš© baseline**
- âš ï¸ ìƒ˜í”Œ íš¨ìœ¨ì„± ì¤‘ê°„

### SAC
- âœ… Continuous control ìµœê°•
- âœ… ìƒ˜í”Œ íš¨ìœ¨ì„± ìµœê³ 
- âœ… ê³ ì°¨ì› ì œì–´
- âŒ Discrete ë¶ˆê°€

## í•µì‹¬ ë©”ì‹œì§€

> **DQN â†’ PPO â†’ SAC**ëŠ” ì„±ëŠ¥ ê²½ìŸì´ ì•„ë‹™ë‹ˆë‹¤.
>
> ê° ì•Œê³ ë¦¬ì¦˜ì€ **ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì œë¥¼ í•´ê²°**í•˜ê¸° ìœ„í•´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤:
>
> - **DQN**: "ë”¥ëŸ¬ë‹ + RL"ì˜ ì‹œì‘
> - **REINFORCE**: "ì •ì±… ì§ì ‘ ìµœì í™”"ì˜ ì§ê´€
> - **PPO**: "ì•ˆì •ì„±"ì˜ í™•ë³´
> - **SAC**: "ìƒ˜í”Œ íš¨ìœ¨ì„± + íƒìƒ‰"ì˜ ê·¹ëŒ€í™”

## ì¶”ê°€ ìë£Œ

- [DQN ìƒì„¸ ê°€ì´ë“œ](dqn.md)
- [REINFORCE ìƒì„¸ ê°€ì´ë“œ](reinforce.md)
- [PPO ìƒì„¸ ê°€ì´ë“œ](ppo.md)
- [SAC ìƒì„¸ ê°€ì´ë“œ](sac.md)
