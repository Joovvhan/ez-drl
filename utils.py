"""ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤"""
import gymnasium as gym
from stable_baselines3 import DQN, A2C, PPO, SAC
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.atari_wrappers import AtariWrapper
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from typing import Optional, Type, Union
import os
from datetime import datetime

from config import (
    TrainingConfig,
    get_default_params,
    is_algo_env_compatible,
    load_config_from_json,
    merge_configs
)


def create_env(config: TrainingConfig):
    """í™˜ê²½ ìƒì„±"""
    # render_mode ì„¤ì •
    render_mode = config.render_mode if config.render else None

    # Atari í™˜ê²½ì¸ ê²½ìš°
    if "ALE/" in config.env_name:
        env = gym.make(config.env_name, render_mode=render_mode)
        env = AtariWrapper(env)
        if config.n_envs > 1:
            raise ValueError("Atari í™˜ê²½ì€ í˜„ì¬ ë‹¨ì¼ í™˜ê²½ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
        return env

    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (ë Œë”ë§ì€ ë³‘ë ¬ í™˜ê²½ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ)
    if config.n_envs > 1:
        if config.render:
            print("ê²½ê³ : ë³‘ë ¬ í™˜ê²½ì—ì„œëŠ” ë Œë”ë§ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë Œë”ë§ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
        # A2C, PPOëŠ” ë³‘ë ¬ í™˜ê²½ ì§€ì›
        if config.algorithm in ["a2c", "ppo"]:
            env = make_vec_env(
                config.env_name,
                n_envs=config.n_envs,
                seed=config.seed,
            )
            return env
        else:
            print(f"ê²½ê³ : {config.algorithm}ì€ ë³‘ë ¬ í™˜ê²½ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¨ì¼ í™˜ê²½ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")

    # ë‹¨ì¼ í™˜ê²½
    env = gym.make(config.env_name, render_mode=render_mode)
    if config.seed is not None:
        env.reset(seed=config.seed)

    return env


def get_algorithm_class(algorithm: str) -> Type[Union[DQN, A2C, PPO, SAC]]:
    """ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ìœ¼ë¡œ í´ë˜ìŠ¤ ë°˜í™˜"""
    algorithms = {
        "dqn": DQN,
        "a2c": A2C,
        "ppo": PPO,
        "sac": SAC,
    }

    if algorithm not in algorithms:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•Œê³ ë¦¬ì¦˜: {algorithm}")

    return algorithms[algorithm]


def create_model(config: TrainingConfig, env, use_json_config: bool = True):
    """ëª¨ë¸ ìƒì„±

    Args:
        config: TrainingConfig ê°ì²´
        env: Gymnasium í™˜ê²½
        use_json_config: JSON ì„¤ì • íŒŒì¼ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """
    # í˜¸í™˜ì„± í™•ì¸
    if not is_algo_env_compatible(config.algorithm, config.env_name):
        raise ValueError(
            f"{config.algorithm}ì€ {config.env_name} í™˜ê²½ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
            f"ì§€ì› ì•Œê³ ë¦¬ì¦˜: {', '.join(config.supported_algos)}"
        )

    # ì•Œê³ ë¦¬ì¦˜ í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    AlgoClass = get_algorithm_class(config.algorithm)

    # íŒŒë¼ë¯¸í„° ìš°ì„ ìˆœìœ„:
    # 1. JSON ì„¤ì • íŒŒì¼ (use_json_config=Trueì¸ ê²½ìš°)
    # 2. ì½”ë“œ ê¸°ë³¸ê°’ (ALGORITHM_DEFAULTS)
    # 3. ì‚¬ìš©ì ì§€ì • (config.algo_params)

    if use_json_config:
        # JSONì—ì„œ ë¡œë“œ
        json_params = load_config_from_json(config.algorithm, config.env_name)
        # ì½”ë“œ ê¸°ë³¸ê°’ê³¼ ë³‘í•©
        default_params = get_default_params(config.algorithm, config.env_name)
        base_params = merge_configs(default_params, json_params)
    else:
        # ì½”ë“œ ê¸°ë³¸ê°’ë§Œ ì‚¬ìš©
        base_params = get_default_params(config.algorithm, config.env_name)

    # ì‚¬ìš©ì ì§€ì • íŒŒë¼ë¯¸í„°ì™€ ë³‘í•© (ìµœìš°ì„ )
    algo_params = merge_configs(base_params, config.algo_params)

    # Policy íƒ€ì… ì„¤ì • (JSONì—ì„œ policy ì§€ì • ê°€ëŠ¥)
    if "policy" in algo_params:
        policy_type = algo_params.pop("policy")
    elif "ALE/" in config.env_name:
        policy_type = "CnnPolicy"
    else:
        policy_type = config.policy_type

    # ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì„¤ì •
    if config.net_arch is not None:
        policy_kwargs = {"net_arch": config.net_arch}
    else:
        policy_kwargs = {}

    # total_timestepsëŠ” configì—ì„œ ê´€ë¦¬ (algo_paramsì—ì„œ ì œê±°)
    algo_params.pop("total_timesteps", None)

    # ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•  íŒŒë¼ë¯¸í„° (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ algo_paramsì—ì„œ ì œê±°)
    if "learning_rate" not in algo_params:
        algo_params["learning_rate"] = config.learning_rate
    if "verbose" not in algo_params:
        algo_params["verbose"] = 1
    if "tensorboard_log" not in algo_params:
        algo_params["tensorboard_log"] = config.tb_log_dir
    if config.seed is not None:
        algo_params["seed"] = config.seed
    if policy_kwargs:
        algo_params["policy_kwargs"] = policy_kwargs

    # ëª¨ë¸ ìƒì„±
    model = AlgoClass(
        policy=policy_type,
        env=env,
        **algo_params,
    )

    return model


def create_callbacks(config: TrainingConfig, eval_env=None):
    """ì½œë°± ìƒì„±"""
    callbacks = []

    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=max(10000, config.total_timesteps // 10),
        save_path=config.results_dir,
        name_prefix=f"{config.algorithm}_checkpoint",
        save_replay_buffer=config.algorithm in ["dqn", "sac"],
        save_vecnormalize=True,
    )
    callbacks.append(checkpoint_callback)

    # í‰ê°€ ì½œë°± (ì„ íƒì‚¬í•­)
    if eval_env is not None:
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=config.results_dir,
            log_path=config.results_dir,
            eval_freq=max(5000, config.total_timesteps // 20),
            n_eval_episodes=config.eval_episodes,
            deterministic=True,
            render=False,
        )
        callbacks.append(eval_callback)

    return callbacks


def train_model(config: TrainingConfig, use_json_config: bool = True):
    """ëª¨ë¸ í•™ìŠµ

    Args:
        config: TrainingConfig ê°ì²´
        use_json_config: JSON ì„¤ì • íŒŒì¼ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """
    # TensorBoard ë¡œê·¸ ì´ë¦„ ìƒì„± (ë‚ ì§œ/ì‹œê°„ë§Œ, ì•Œê³ ë¦¬ì¦˜ ì´ë¦„ì€ í´ë” êµ¬ì¡°ì— ì´ë¯¸ í¬í•¨)
    tb_log_name = datetime.now().strftime('%Y%m%d_%H%M%S')

    print("=" * 70)
    print(f"ğŸš€ {config.algorithm.upper()} í•™ìŠµ ì‹œì‘: {config.env_name}")
    print("=" * 70)
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {config.results_dir}")
    print(f"â±ï¸  ì´ Timesteps: {config.total_timesteps:,}")
    print(f"ğŸ”§ ë³‘ë ¬ í™˜ê²½ ìˆ˜: {config.n_envs}")
    if config.tensorboard_log:
        print(f"ğŸ“Š TensorBoard ë¡œê·¸: {config.tb_log_dir}/{tb_log_name}")
    if use_json_config:
        print(f"âš™ï¸  JSON ì„¤ì • ì‚¬ìš©: configs/{config.algorithm}.json")
    print("=" * 70)

    # í™˜ê²½ ìƒì„±
    env = create_env(config)

    # í‰ê°€ í™˜ê²½ ìƒì„± (ë³‘ë ¬ í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš°)
    eval_env = None
    if config.n_envs == 1 and "ALE/" not in config.env_name:
        eval_env = gym.make(config.env_name)

    # ëª¨ë¸ ìƒì„±
    model = create_model(config, env, use_json_config=use_json_config)

    # ì½œë°± ìƒì„±
    callbacks = create_callbacks(config, eval_env)

    # í•™ìŠµ
    try:
        model.learn(
            total_timesteps=config.total_timesteps,
            callback=callbacks,
            log_interval=config.log_interval,
            progress_bar=True,
            tb_log_name=tb_log_name,
        )
    except KeyboardInterrupt:
        print("\nâš ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ëª¨ë¸ ì €ì¥
    model_path = config.get_model_path("final")
    model.save(model_path)
    print(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}.zip")

    # í‰ê°€
    print("\n" + "=" * 70)
    print("ğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...")
    print("=" * 70)

    if eval_env is None:
        eval_env = gym.make(config.env_name)

    mean_reward, std_reward = evaluate_policy(
        model,
        eval_env,
        n_eval_episodes=config.eval_episodes,
        deterministic=True,
    )

    print(f"\ní‰ê·  ë³´ìƒ: {mean_reward:.2f} +/- {std_reward:.2f}")

    # í™˜ê²½ ì •ë¦¬
    env.close()
    if eval_env is not None:
        eval_env.close()

    return model, mean_reward, std_reward


def test_model(config: TrainingConfig, model_path: Optional[str] = None, n_episodes: int = 3, n_steps: Optional[int] = None):
    """í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸

    Args:
        config: TrainingConfig ê°ì²´
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ìœ¼ë¡œ ì°¾ìŒ)
        n_episodes: í…ŒìŠ¤íŠ¸í•  ì—í”¼ì†Œë“œ ìˆ˜
        n_steps: ê° ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)

    Returns:
        mean_reward, std_reward: í‰ê·  ë³´ìƒê³¼ í‘œì¤€í¸ì°¨
    """
    if model_path is None:
        model_path = config.get_model_path("final")

    # ëª¨ë¸ ë¡œë“œ
    AlgoClass = get_algorithm_class(config.algorithm)
    model = AlgoClass.load(model_path)

    # í™˜ê²½ ìƒì„± (ë Œë”ë§ ëª¨ë“œ)
    render_mode = "human" if config.render else None
    if "ALE/" in config.env_name:
        env = gym.make(config.env_name, render_mode=render_mode)
        env = AtariWrapper(env)
    else:
        env = gym.make(config.env_name, render_mode=render_mode)

    print(f"\nğŸ® í•™ìŠµëœ ëª¨ë¸ë¡œ {n_episodes}ê°œ ì—í”¼ì†Œë“œ ì‹¤í–‰ ì¤‘...")
    if n_steps:
        print(f"   (ê° ì—í”¼ì†Œë“œ ìµœëŒ€ {n_steps} ìŠ¤í…ìœ¼ë¡œ ì œí•œ)")
    else:
        print(f"   (ìŠ¤í… ì œí•œ ì—†ìŒ - ì—í”¼ì†Œë“œê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì¢…ë£Œë  ë•Œê¹Œì§€)")
    print("=" * 70)

    episode_rewards = []

    for episode in range(n_episodes):
        obs, _ = env.reset()
        done = False
        total_reward = 0
        steps = 0
        force_stopped = False

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = env.step(action)
            total_reward += reward
            steps += 1

            # ìµœëŒ€ ìŠ¤í… ì œí•œ ì²´í¬
            if n_steps and steps >= n_steps:
                done = True
                force_stopped = True
            else:
                done = terminated or truncated

        episode_rewards.append(total_reward)

        # ì¢…ë£Œ ì›ì¸ í‘œì‹œ
        if force_stopped:
            status = f" (ê°•ì œ ì¢…ë£Œ: {n_steps} ìŠ¤í… ë„ë‹¬)"
        elif steps < (n_steps or float('inf')):
            status = " (ìì—° ì¢…ë£Œ)"
        else:
            status = ""

        print(f"Episode {episode + 1}: Reward = {total_reward:.2f}, Steps = {steps}{status}")

    env.close()

    # í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
    import numpy as np
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    print("=" * 70)
    print(f"ğŸ“Š í‰ê·  ë³´ìƒ: {mean_reward:.2f} Â± {std_reward:.2f}")
    print("=" * 70)

    return mean_reward, std_reward


def print_available_configs():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í™˜ê²½ê³¼ ì•Œê³ ë¦¬ì¦˜ ì¶œë ¥"""
    from config import ENVIRONMENT_CONFIGS

    print("\n" + "=" * 70)
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í™˜ê²½ ë° ì•Œê³ ë¦¬ì¦˜")
    print("=" * 70)

    for env_name, env_config in ENVIRONMENT_CONFIGS.items():
        print(f"\nğŸ® {env_name}")
        print(f"   - Action Space: {env_config['action_space']}")
        print(f"   - State Space: {env_config['state_space']}")
        print(f"   - Success Threshold: {env_config['success_threshold']}")
        print(f"   - Supported Algorithms: {', '.join(env_config['supported_algos'])}")

    print("\n" + "=" * 70)
